
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>CHEBFUN GUIDE 6: QUASIMATRICES AND LEAST-SQUARES</title><meta name="generator" content="MATLAB 7.14"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2012-03-30"><meta name="DC.source" content="guide6.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, tt, code { font-size:12px; }
pre { margin:0px 0px 20px; }
pre.error { color:red; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }

  </style></head><body><div class="content"><h1>CHEBFUN GUIDE 6: QUASIMATRICES AND LEAST-SQUARES</h1><!--introduction--><p>Lloyd N. Trefethen, November 2009, revised Feburary 2011</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">6.1  Quasimatrices and "spy"</a></li><li><a href="#10">6.2 Backslash and least-squares</a></li><li><a href="#17">6.3 QR factorization</a></li><li><a href="#26">6.4 SVD, norm, cond</a></li><li><a href="#45">6.5 Other norms</a></li><li><a href="#50">6.6 rank, null, orth, pinv</a></li><li><a href="#56">6.7  References</a></li></ul></div><h2>6.1  Quasimatrices and "spy"<a name="1"></a></h2><p>A chebfun can have more than one column, or if it is transposed, it can have more than one row.  In these cases we get a <b>quasimatrix</b>, a "matrix" in which one of the dimensions is discrete as usual but the other is continuous.  Our default choice will be that of an "infinity by n" quasimatrix consisting of n columns, each of which is a chebfun.  When it is important to specify the orientation we use the term <b>column quasimatrix</b> or <b>row quasimatrix</b>.</p><p>Here for example is the quasimatrix consisting of the first six powers of x on the interval [-1,1].  The command "size" can be used to identify the continuous dimension, and to find the numbers of rows or columns:</p><pre class="codeinput">  x = chebfun(<span class="string">'x'</span>);
  A = [1 x x.^2 x.^3 x.^4 x.^5];
  size(A)
  size(A,2)
</pre><pre class="codeoutput">ans =
   Inf     6
ans =
     6
</pre><p>Here is the third column of A evaluated at the point x=0.5:</p><pre class="codeinput">  A(0.5,3)
</pre><pre class="codeoutput">ans =
   0.250000000000000
</pre><p>Here are the column sums, i.e., the integrals of 1, x,..., x^5 from -1 to 1:</p><pre class="codeinput">  format <span class="string">short</span>, sum(A), format <span class="string">long</span>
</pre><pre class="codeoutput">ans =
    2.0000         0    0.6667         0    0.4000         0
</pre><p>And here is a plot of the columns:</p><pre class="codeinput">  plot(A), grid <span class="string">on</span>
</pre><img vspace="5" hspace="5" src="guide6_01.png" alt=""> <p>The term quasimatrix comes from [Stewart 1998], and the same idea appears with different terminology in [de Boor 1991] and [Trefethen &amp; Bau 1997, pp. 52-54].  The idea is a natural one, since so much of applied linear algebra deals with discrete approximations to the continuous, but it seems not to have been discussed explicitly very much until the appearance of Chebfun [Battles &amp; Trefethen 2004, Battles 2006].</p><p>If f and g are column chebfuns, then f'*g is a scalar, their inner product. For example, here is the inner product of x^2 and x^4 over [-1,1] (equal to 2/7):</p><pre class="codeinput">  A(:,3)'*A(:,5)
</pre><pre class="codeoutput">ans =
   0.285714285714286
</pre><p>More generally, if A and B are column quasimatrices with m and n columns, respectively, then A'*B is the m x n matrix of inner products of those columns.  Here is the 6x6 example corresponding to B=A:</p><pre class="codeinput">  format <span class="string">short</span>, A'*A, format <span class="string">long</span>
</pre><pre class="codeoutput">ans =
    2.0000   -0.0000    0.6667    0.0000    0.4000    0.0000
   -0.0000    0.6667    0.0000    0.4000   -0.0000    0.2857
    0.6667    0.0000    0.4000    0.0000    0.2857   -0.0000
    0.0000    0.4000    0.0000    0.2857   -0.0000    0.2222
    0.4000   -0.0000    0.2857   -0.0000    0.2222   -0.0000
    0.0000    0.2857   -0.0000    0.2222   -0.0000    0.1818
</pre><p>You can get an idea of the shape of a quasimatrix with the overloaded SPY command</p><pre class="codeinput">  subplot(1,2,1), spy(A), title <span class="string">A</span>
  subplot(1,2,2), spy(A'), title(<span class="string">'A'''</span>)
</pre><img vspace="5" hspace="5" src="guide6_02.png" alt=""> <h2>6.2 Backslash and least-squares<a name="10"></a></h2><p>In Matlab, the command c = A\b computes the solution to the system of equations Ac = b if A is a square matrix, whereas if A is rectangular, with more rows than columns, it computes the least squares solution, the vector c that minimizes norm(A*c-b).  A quasimatrix is always rectangular, and "\" has accordingly been overloaded to carry out the appropriate continuous least-squares computation. (The actual Matlab command that handles backslash is called mldivide.)</p><p>For example, continuing with the same chebfun x and quasimatrix A as above, consider the following sequence:</p><pre class="codeinput">  f = exp(x).*sin(6*x);
  c = A\f
</pre><pre class="codeoutput">c =
   0.309654988398407
   4.640757102742477
  -2.157249816336411
 -20.041645425109216
   1.073963006923387
  15.477982292828056
</pre><p>The vector c can be interpreted as the vector of coefficients of the least-squares fit to f by a linear combination of the functions 1, x,..., x^5.  Here is a plot of f (in blue) and the least-squares approximation (in red), which we label ffit.</p><pre class="codeinput">  ffit = A*c;
  clf, plot(f), grid <span class="string">on</span>, hold <span class="string">on</span>
  plot(ffit,<span class="string">'r'</span>), hold <span class="string">off</span>
  error = norm(f-ffit)
</pre><pre class="codeoutput">error =
   0.356073976001434
</pre><img vspace="5" hspace="5" src="guide6_03.png" alt=""> <p>It is a general result that the least-squares approximation by a polynomial of degree n to a continuous function f must intersect f at least n+1 times in the interval of approximation.</p><p>Here is quite a different quasimatrix whose columns can be used to fit f. The columns correspond to hat functions located at points equally spaced from -1 to 1, and they are realized as piecewise smooth chebfuns.</p><pre class="codeinput">  A2 = [];
  <span class="keyword">for</span> j = 0:8
    xj = -1 + j/4;
    A2 = [A2 max(0,1-4*abs(x-xj))];
  <span class="keyword">end</span>
  plot(A2)
  set(gca,<span class="string">'xtick'</span>,-1:.25:1)
</pre><img vspace="5" hspace="5" src="guide6_04.png" alt=""> <p>A linear combination of these columns is a piecewise linear function with breakpoints at -0.75, -0.50,...,0.75.  Here is the least-squares fit by such functions to f.  Remember that although we happen to be fitting here by a function with a discrete flavor, all the operations are continuous ones involving integrals, not point evaluations.</p><pre class="codeinput">  c = A2\f;
  ffit = A2*c;
  plot(f), grid <span class="string">on</span>, hold <span class="string">on</span>
  plot(ffit,<span class="string">'.-r'</span>), hold <span class="string">off</span>
  set(gca,<span class="string">'xtick'</span>,-1:.25:1)
  error = norm(f-ffit)
</pre><pre class="codeoutput">error =
   0.148137345378415
</pre><img vspace="5" hspace="5" src="guide6_05.png" alt=""> <h2>6.3 QR factorization<a name="17"></a></h2><p>Matrix least-squares problems are ordinarily solved by QR factorization, and in the quasimatrix case, they are solved by quasimatrix QR factorization. This is the technology underlying the backslash operator described in the last section.</p><p>A quasimatrix QR factorization takes this form:</p><pre>      A = QR,  where A is inf x n,  Q is inf x n,  R is n x n.</pre><p>The columns of A are arbitrary, the columns of Q are orthonormal, and R is an n x n upper-triangular matrix.  This factorization corresponds to what is known in various texts as the "reduced", "economy size", "skinny", "abbreviated", or "condensed" QR factorization, since Q is rectangular rather than square and R is square rather than rectangular.  In Matlab the syntax for computing such things is [Q,R] = qr(A,0), and the same command has been overloaded for chebfuns.  The computation makes use of a quasimatrix analogue of Householder triangularization [Trefethen 2010].  Alternatively one can simply write [Q,R] = qr(A):</p><pre class="codeinput">  [Q,R] = qr(A);
  plot(Q), grid <span class="string">on</span>
</pre><img vspace="5" hspace="5" src="guide6_06.png" alt=""> <p>The spy command confirms the shape of these various matrices. One sees fewer dots in the spy plot than one may expect at first, the reason being that half its entries in the upper-triangle should be zero because of the fact that the columns of A alternate even and odd functions. In fact, because of rounding or truncation errors, not all those mathematical zeros are zero on the computer, so the upper half of spy (R) appears as an imperfect checkerboard.</p><pre class="codeinput">  subplot(1,3,1), spy(A), title <span class="string">A</span>
  subplot(1,3,2), spy(Q), title <span class="string">Q</span>
  subplot(1,3,3), spy(R), title <span class="string">R</span>
</pre><img vspace="5" hspace="5" src="guide6_07.png" alt=""> <p>The plot shows <b>orthogonal polynomials</b>, namely the orthogonalizations of the monomials 1, x,...,x^5 over [-1,1].  These are the famous Legendre polynomials {P_k} [Abramowitz &amp; Stegun 1972], except that the latter are conventionally normalized by the condition P(1) = 1 rather than by having norm 1.  We can renormalize to impose this condition as follows:</p><pre class="codeinput">  <span class="keyword">for</span> j = 1:size(A,2)
    R(j,:) = R(j,:)*Q(1,j);
    Q(:,j) = Q(:,j)/Q(1,j);
  <span class="keyword">end</span>
  clf, plot(Q), grid <span class="string">on</span>
</pre><img vspace="5" hspace="5" src="guide6_08.png" alt=""> <p>(A slicker way to produce this plot in Chebfun would be simply to type plot(legpoly(0:5)).)</p><p>If A=QR, then A*inv (R) = Q, and here is inv (R):</p><pre class="codeinput">  format <span class="string">short</span>, inv(R), format <span class="string">long</span>
</pre><pre class="codeoutput">ans =
    1.0000    0.0000   -0.5000   -0.0000    0.3750    0.0000
         0    1.0000    0.0000   -1.5000    0.0000    1.8750
         0         0    1.5000    0.0000   -3.7500    0.0000
         0         0         0    2.5000   -0.0000   -8.7500
         0         0         0         0    4.3750   -0.0000
         0         0         0         0         0    7.8750
</pre><p>The kth column of inv (R) is the vector of coefficients for the expansion of the kth column of Q as a linear combination of the columns of A, that is, the monomials 1, x, x^2,....  In other words, the kth column of inv (R) is the vector of coefficients of the kth Legendre polynomial.  For example, we see from the matrix that P_3(x) = 2.5x^3 - 1.5x.</p><p>Here is what the hat functions look like after orthonormalization:</p><pre class="codeinput">  [Q2,R2] = qr(A2);
  plot(Q2)
</pre><img vspace="5" hspace="5" src="guide6_09.png" alt=""> <h2>6.4 SVD, norm, cond<a name="26"></a></h2><p>An m x n matrix A defines a map from R^n to R^m, and in particular, A maps the unit ball in R^n to a hyperellipsoid of dimension &lt;=n in R^m.  The (reduced, skinny, condensed,...) <b>SVD</b> or <b>singular value decomposition</b> exhibits this map by providing a factorization  AV = US or equivalently A = USV', where U is m x n with orthonormal columns, S is diagonal with nonincreasing nonnegative diagonal entries known as the <b>singular values</b>, and V is n x n and orthogonal. A maps v_j, the jth column of V or jth <b>right singular vector</b>, to s_j times u_j, the jth column of U or jth <b>left singular vector</b>, which is the vector defining the jth largest semiaxis of the hyperellipsoid.  See Chapters 4 and 5 of [Trefethen &amp; Bau 1997].</p><p>If A is an inf x n quasimatrix, everything is analogous:</p><pre>    A = USV',  where A is inf x n,  U is inf x n,  S is n x n,  V is n x n.</pre><p>The image of the unit ball in R^n under A is still a hyperellipsoid of dimension &lt;=n, which now lies within an infinite-dimensional function space. The columns of Q are orthonormal functions and S and V have the same properties as in the matrix case.</p><p>For example, here are the singular values of the matrix A defined earlier with columns 1,x,...,x^5:</p><pre class="codeinput">  s = svd(A,0)
</pre><pre class="codeoutput">s =
   1.532062889375340
   1.032551897396699
   0.518125864967968
   0.258419769500035
   0.080938947808205
   0.035425077461572
</pre><p>The largest singular value is equal to the norm of the quasimatrix, which is defined by  norm(A,2) = max_x norm(A*x)/norm(x).</p><pre class="codeinput">  norm(A,2)
</pre><pre class="codeoutput">ans =
   1.532062889375340
</pre><p>(Note that we must include the argument "2" here: for reasons of speed, the default for quasimatrices, unlike the usual Matlab matrices, is the Frobenius norm rather than the 2-norm.) The SVD enables us to identify exactly what vectors are involved in achieving this maximum ratio.  The optimal vector x is v1, the first right singular vector of A,</p><pre class="codeinput">  [U,S,V] = svd(A);
</pre><p>First we use spy to confirm the shapes of the matrices.  As with spy (R) earlier, here spy(V) should in principle show a checkerboard, but some of its blanks are turned into dots by rounding or truncation errors.</p><pre class="codeinput">  subplot(1,4,1), spy(A), title <span class="string">A</span>
  subplot(1,4,2), spy(U), title <span class="string">U</span>
  subplot(1,4,3), spy(S), title <span class="string">S</span>
  subplot(1,4,4), spy(V), title <span class="string">V</span>
  v1 = V(:,1)
</pre><pre class="codeoutput">v1 =
   0.913034433780914
  -0.000000000000000
   0.344611116356111
  -0.000000000000000
   0.218200140270717
  -0.000000000000000
</pre><img vspace="5" hspace="5" src="guide6_10.png" alt=""> <p>Next we confirm that the norm of v1 is 1:</p><pre class="codeinput">  norm(v1)
</pre><pre class="codeoutput">ans =
   1.000000000000000
</pre><p>This vector is mapped by A to the chebfun s1*u1:</p><pre class="codeinput">  u1 = U(:,1);
  norm(u1)
</pre><pre class="codeoutput">ans =
   1.000000000000000
</pre><pre class="codeinput">  s1 = S(1,1)
</pre><pre class="codeoutput">s1 =
   1.532062889375340
</pre><pre class="codeinput">  norm(A*v1)
</pre><pre class="codeoutput">ans =
   1.532062889375340
</pre><pre class="codeinput">  norm(A*v1-s1*u1)
</pre><pre class="codeoutput">ans =
     8.763429766539204e-16
</pre><p>Similarly, the minimal singular value and corresponding singular vectors describe the minimum amount that A can enlarge an input.  The following commands plot the extreme functions A*v1 (blue) and A*vn (red).  We can interpret these as the largest and smallest degree-5 polynomials, as measured in the 2-norm over [-1,1], whose coefficient vectors have 2-norm equal to 1.</p><pre class="codeinput">  clf, plot(A*v1), grid <span class="string">on</span>, hold <span class="string">on</span>
  vn = V(:,end);
  plot(A*vn,<span class="string">'r'</span>), hold <span class="string">off</span>
</pre><img vspace="5" hspace="5" src="guide6_11.png" alt=""> <p>The ratio of the largest and smallest singular values -- the eccentricity of the hyperellipsoid -- is the condition number of A:</p><pre class="codeinput">  max(s)/min(s)
</pre><pre class="codeoutput">ans =
  43.247975704139805
</pre><pre class="codeinput">  cond(A)
</pre><pre class="codeoutput">ans =
  43.247975704139805
</pre><p>The fact that cond(A) is a good deal greater than 1 reflects the ill-conditioning of the monomials 1,x,...,x^5 as a basis for polynomials in [-1,1].  The effect becomes rapidly stronger as we take more terms in the sequence:</p><pre class="codeinput">  cond([1 x x.^2 x.^3 x.^4 x.^5 x.^6 x.^7 x.^8 x.^9 x.^10])
</pre><pre class="codeoutput">ans =
     3.072959852624408e+03
</pre><p>By contrast a quasimatrix formed of suitably normalized Legendre polynomials has condition number 1, since they are orthonormal:</p><pre class="codeinput">cond(legpoly(0:10,<span class="string">'norm'</span>))
</pre><pre class="codeoutput">ans =
   1.000000000000001
</pre><p>A quasimatrix of Chebyshev polynomials doesn't quite achieve condition number 1, but it comes close:</p><pre class="codeinput">cond(chebpoly(0:10))
</pre><pre class="codeoutput">ans =
   3.712641510134901
</pre><p>Chebyshev polynomials form an excellent basis for expansions on [-1,1], a fact that is the starting point of Chebfun.</p><h2>6.5 Other norms<a name="45"></a></h2><p>The definition  norm(A) = max_x norm(A*x)/norm(x) makes sense in other norms besides the 2-norm, and the particularly important alternatives are the 1-norm and the inf-norm.  The 1-norm of a column quasimatrix is the "maximum column sum", i.e., the maximum of the 1-norms of its columns.   In the case of our quasimatrix A, the maximum is attained by the first column, which has norm 2:</p><pre class="codeinput">  norm(A,1)
</pre><pre class="codeoutput">ans =
   1.938148951041007
</pre><p>The inf-norm is the "maximum row sum", which for a column quasimatrix corresponds to the maximum of the chebfun obtained by adding the absolute values of the columns.  In the case of A, the sum is 1+|x|+...+|x|^5, which attains its maximum value 6 at x=-1 and 1:</p><pre class="codeinput">  norm(A,inf)
</pre><pre class="codeoutput">ans =
     6
</pre><p>The norms of row quasimatrices are analogous, with norm(A',inf) = norm(A,1) and norm(A',1) = norm(A,inf). Like Matlab itself applied to a rectangular matrix, Chebfun does not define cond(A,1) or cond(A,inf) if A is a quasimatrix.</p><p>The Frobenius or Hilbert-Schmidt norm is equal to the square root of the sum of the squares of the singular values:</p><pre class="codeinput">  norm(A,<span class="string">'fro'</span>)
</pre><pre class="codeoutput">ans =
   1.938148951041007
</pre><h2>6.6 rank, null, orth, pinv<a name="50"></a></h2><p>Chebfun also contains overloads for some further Matlab operations related to orthogonal matrix factorizations. Perhaps the most useful of these is rank(A), which computes the singular values of A and makes a judgement as to how many of them are significantly different from zero.  For example, with x still defined as before, here is an example showing that the functions 1, sin(x)^2, and cos(x)^2 are linearly dependent:</p><pre class="codeinput">  B = [1 sin(x).^2 cos(x).^2];
  rank(B)
</pre><pre class="codeoutput">ans =
     2
</pre><p>Since B is rank-deficient, is has a nontrivial nullspace, and the command null(B) will find an orthonormal basis for it:</p><pre class="codeinput">  null(B)
</pre><pre class="codeoutput">ans =
  -0.577350269189626
   0.577350269189626
   0.577350269189626
</pre><p>Similarly the command orth(B) finds an orthonormal basis for the range of B, which in this case has dimension 2:</p><pre class="codeinput">  orth(B)
</pre><pre class="codeoutput">ans = 
   chebfun column 1 (1 smooth piece)
       interval       length   endpoint values   
[      -1,       1]       17     0.61     0.61   
vertical scale = 0.76 
   chebfun column 2 (1 smooth piece)
       interval       length   endpoint values   
[      -1,       1]       17     -1.4     -1.4   
vertical scale = 1.4 
</pre><p>If A is an inf x n column quasimatrix, the command pinv(A) computes the pseudoinverse of A, an n x inf row quasimatrix such that pinv(A)*c = A\c.</p><p>Here is a summary of the dimensions of these objects:</p><pre class="codeinput">  subplot(1,3,1), spy(null(B)), title <span class="string">null(B)</span>
  subplot(1,3,2), spy(orth(B)), title <span class="string">orth(B)</span>
  subplot(1,3,3), spy(pinv(A)), title <span class="string">pinv(A)</span>
</pre><img vspace="5" hspace="5" src="guide6_12.png" alt=""> <h2>6.7  References<a name="56"></a></h2><p>[Abramowitz &amp; Stegun 1972] M. A. Abramowitz and I. A. Stegun, eds., Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th printing, Dover, 1972.</p><p>[Battles 2006] Z. Battles, Numerical Linear Algebra for Continuous Functions, DPhil thesis, Oxford University Computing Laboratory, 2006.</p><p>[Battles &amp; Trefethen 2004] Z. Battles and L. N. Trefethen, "An extension of Matlab to continuous functions and operators", SIAM Journal on Scientific Computing 25 (2004), 1743-1770.</p><p>[de Boor 1991] C. de Boor, "An alternative approach to (the teaching of) rank, basis, and dimension", Linear Algebra and its Applications 146 (1991), 221-229.</p><p>[Stewart 1998] G. W. Stewart, Afternotes Goes to Graduate School: Lectures on Advanced Numerical Analysis, SIAM, 1998.</p><p>[Trefethen 2008] L. N. Trefethen, "Householder triangularization of a quasimatrix", IMA Journal on Numerical Analysis 30 (2010), 887-897.</p><p>[Trefethen &amp; Bau 1997] L. N. Trefethen and D. Bau, III, Numerical Linear Algebra, SIAM, 1997.</p><p class="footer"><br>
      Published with MATLAB&reg; 7.14<br></p></div><!--
##### SOURCE BEGIN #####
%% CHEBFUN GUIDE 6: QUASIMATRICES AND LEAST-SQUARES
% Lloyd N. Trefethen, November 2009, revised Feburary 2011

%% 6.1  Quasimatrices and "spy"
% A chebfun can have more than one column, or if it is transposed, it can
% have more than one row.  In these cases we get a
% *quasimatrix*, a "matrix" in which one of the dimensions is
% discrete as usual but the other is continuous.  Our default choice
% will be that of an "infinity by n" quasimatrix consisting of n columns,
% each of which is a chebfun.  When it is important to specify the orientation
% we use the term *column quasimatrix* or *row quasimatrix*.

%%
% Here for example is the quasimatrix consisting of the first
% six powers of x on the interval [-1,1].  The command "size" can be
% used to identify the continuous dimension, and to find the numbers
% of rows or columns:

  x = chebfun('x');
  A = [1 x x.^2 x.^3 x.^4 x.^5];
  size(A)
  size(A,2)

%%
% Here is the third column of A evaluated at the point x=0.5:

  A(0.5,3)

%%
% Here are the column sums, i.e., the integrals of 1, x,..., x^5
% from -1 to 1:

  format short, sum(A), format long

%%
% And here is a plot of the columns:

  plot(A), grid on

%%
% The term quasimatrix comes from [Stewart 1998], and the
% same idea appears with different terminology in [de Boor 1991] and
% [Trefethen & Bau 1997, pp. 52-54].  The idea is a natural one,
% since so much of applied linear algebra
% deals with discrete approximations to the continuous, but it seems not to have
% been discussed explicitly very much until the appearance of
% Chebfun [Battles & Trefethen 2004, Battles 2006].

%%
% If f and g are column chebfuns, then f'*g is a scalar, their inner product.
% For example, here is the inner product of x^2 and x^4 over [-1,1] (equal to 2/7):

  A(:,3)'*A(:,5)

%%
% More generally, if A and B are column quasimatrices with m and n columns, respectively,
% then A'*B is the m x n matrix of inner products of those columns.  Here is the
% 6x6 example corresponding to B=A:

  format short, A'*A, format long

%%
% You can get an idea of the shape of a quasimatrix with the overloaded SPY command

  subplot(1,2,1), spy(A), title A
  subplot(1,2,2), spy(A'), title('A''')

%% 6.2 Backslash and least-squares

%%
% In Matlab, the command c = A\b computes the solution to the system
% of equations Ac = b if A is a square matrix, whereas if A is rectangular,
% with more rows than columns, it computes the least squares solution,
% the vector c that minimizes norm(A*c-b).  A quasimatrix is always rectangular, and
% "\" has accordingly been overloaded
% to carry out the appropriate continuous least-squares computation.
% (The actual Matlab command that handles backslash is called mldivide.)

%%
% For example, continuing with the same chebfun x and
% quasimatrix A as above, consider the following sequence:

  f = exp(x).*sin(6*x);
  c = A\f
  
%%
% The vector c can be interpreted as the vector of
% coefficients of the least-squares fit to f by a linear combination
% of the functions 1, x,..., x^5.  Here is a plot of f (in blue) and
% the least-squares approximation (in red), which we label ffit.

  ffit = A*c;
  clf, plot(f), grid on, hold on
  plot(ffit,'r'), hold off
  error = norm(f-ffit)

%%
% It is a general result that the least-squares approximation 
% by a polynomial of degree n to a continuous function f must intersect
% f at least n+1 times in the interval of approximation.
%
%%
% Here is quite a different quasimatrix whose columns can be used to fit f.
% The columns correspond to hat functions located at points equally
% spaced from -1 to 1, and they are realized as piecewise smooth chebfuns.

  A2 = [];
  for j = 0:8
    xj = -1 + j/4;
    A2 = [A2 max(0,1-4*abs(x-xj))];
  end
  plot(A2)
  set(gca,'xtick',-1:.25:1)

%%
% A linear combination of these columns is a piecewise linear function
% with breakpoints at -0.75, -0.50,...,0.75.  Here is the least-squares
% fit by such functions to f.  Remember that although we happen to be
% fitting here by a function with a discrete flavor, all the operations
% are continuous ones involving integrals, not point evaluations.

  c = A2\f;
  ffit = A2*c;
  plot(f), grid on, hold on
  plot(ffit,'.-r'), hold off
  set(gca,'xtick',-1:.25:1)
  error = norm(f-ffit)
    

%% 6.3 QR factorization

%%
% Matrix least-squares problems are ordinarily solved by QR factorization,
% and in the quasimatrix case, they are solved by quasimatrix QR factorization.
% This is the technology underlying the backslash operator described in the
% last section.

%%
% A quasimatrix QR factorization takes this form:
%
%        A = QR,  where A is inf x n,  Q is inf x n,  R is n x n.
%
% The columns of A are arbitrary, the columns of Q are orthonormal, and
% R is an n x n upper-triangular matrix.  This factorization corresponds
% to what is known in various texts as the "reduced", "economy size", "skinny",
% "abbreviated", or "condensed"
% QR factorization, since Q is rectangular rather than square and R is
% square rather than rectangular.  In Matlab the syntax for computing such things is
% [Q,R] = qr(A,0), and the same command has been overloaded for chebfuns.  The
% computation makes use of a quasimatrix analogue of Householder triangularization
% [Trefethen 2010].  Alternatively one can simply write [Q,R] = qr(A):

  [Q,R] = qr(A);
  plot(Q), grid on

%%
% The spy command confirms the shape of these various matrices.
% One sees fewer dots in the spy plot than one may expect at first,
% the reason being that half its entries in the upper-triangle should
% be zero because of the fact that
% the columns of A alternate even and odd functions.
% In fact, because of rounding or truncation errors, not all those mathematical
% zeros are zero on the computer, so the upper half of
% spy (R) appears as an imperfect checkerboard.

  subplot(1,3,1), spy(A), title A
  subplot(1,3,2), spy(Q), title Q
  subplot(1,3,3), spy(R), title R

%%
% The plot shows *orthogonal polynomials*, namely the orthogonalizations of
% the monomials 1, x,...,x^5 over [-1,1].  These are the famous Legendre
% polynomials {P_k} [Abramowitz & Stegun 1972],
% except that the latter are conventionally normalized by the
% condition P(1) = 1 rather than by having norm 1.  We can renormalize to
% impose this condition as follows:

  for j = 1:size(A,2)
    R(j,:) = R(j,:)*Q(1,j);
    Q(:,j) = Q(:,j)/Q(1,j);
  end
  clf, plot(Q), grid on

%%
% (A slicker way to produce this plot in Chebfun would be
% simply to type plot(legpoly(0:5)).)
  
%%
% If A=QR, then A*inv (R) = Q, and here is inv (R):
  format short, inv(R), format long

%%
% The kth column of inv (R) is the vector of coefficients for the 
% expansion of the kth column of Q as a linear combination of the
% columns of A, that is, the monomials 1, x, x^2,....  In other words,
% the kth column of inv (R) is the vector of coefficients of the kth
% Legendre polynomial.  For example, we see from the matrix
% that P_3(x) = 2.5x^3 - 1.5x.

%%
% Here is what the hat functions look like after orthonormalization:

  [Q2,R2] = qr(A2);
  plot(Q2)

%% 6.4 SVD, norm, cond

%%
% An m x n matrix A defines a map from R^n to R^m, and in particular, A maps the unit ball in
% R^n to a hyperellipsoid of dimension <=n in R^m.  The (reduced, skinny, condensed,...)
% *SVD* or *singular value decomposition* exhibits this
% map by providing a factorization  AV = US or equivalently
% A = USV', where U is m x n with orthonormal columns, S is diagonal with
% nonincreasing nonnegative diagonal entries known as the *singular values*,
% and V is n x n and orthogonal.
% A maps v_j, the jth column of V or jth *right singular vector*, to s_j times
% u_j, the jth column of U or jth *left singular vector*, which is the vector
% defining the jth largest semiaxis of the hyperellipsoid.  See Chapters 4 and 5
% of [Trefethen & Bau 1997].

%%
% If A is an inf x n quasimatrix, everything is analogous:
%
%      A = USV',  where A is inf x n,  U is inf x n,  S is n x n,  V is n x n.
%
% The image of the unit ball in R^n under A is still a hyperellipsoid of dimension <=n, which now
% lies within an infinite-dimensional function space.
% The columns of Q are orthonormal functions and S and V have the same properties
% as in the matrix case.

%%
% For example, here are the singular values of the matrix
% A defined earlier with columns 1,x,...,x^5:

  s = svd(A,0)

%%
% The largest singular value is equal to the norm of the quasimatrix, which
% is defined by  norm(A,2) = max_x norm(A*x)/norm(x).

  norm(A,2)

%%
% (Note that we must include the argument "2" here: for reasons of speed,
% the default for quasimatrices, unlike the usual Matlab matrices, is
% the Frobenius norm rather than the 2-norm.)
% The SVD enables us to identify exactly what vectors are involved in achieving
% this maximum ratio.  The optimal vector x is v1, the first right singular vector of A,

  [U,S,V] = svd(A);

%%
% First we use spy to confirm the shapes of the matrices.  As with spy (R) earlier, here
% spy(V) should in principle show a checkerboard, but some of its blanks are turned into
% dots by rounding or truncation errors.

  subplot(1,4,1), spy(A), title A
  subplot(1,4,2), spy(U), title U
  subplot(1,4,3), spy(S), title S
  subplot(1,4,4), spy(V), title V
  v1 = V(:,1)

%%
% Next we confirm that the norm of v1 is 1:

  norm(v1)

%%
% This vector is mapped by A to the chebfun s1*u1:

  u1 = U(:,1);
  norm(u1)

%%

  s1 = S(1,1)

%%
  norm(A*v1)

%%
  norm(A*v1-s1*u1)


%%
% Similarly, the minimal singular value and corresponding singular vectors
% describe the minimum amount that A can enlarge an input.  The following
% commands plot the extreme functions A*v1 (blue) and A*vn (red).  We can 
% interpret these as the largest and smallest degree-5 polynomials, as
% measured in the 2-norm over [-1,1], whose coefficient vectors have 2-norm equal to 1.

  clf, plot(A*v1), grid on, hold on
  vn = V(:,end);
  plot(A*vn,'r'), hold off

%%
% The ratio of the largest and smallest singular values REPLACE_WITH_DASH_DASH the
% eccentricity of the hyperellipsoid REPLACE_WITH_DASH_DASH is the condition number of A:

  max(s)/min(s)

%%
  cond(A)

%%
% The fact that cond(A) is a good deal greater than 1 reflects the ill-conditioning
% of the monomials 1,x,...,x^5 as a basis for polynomials in [-1,1].  The effect
% becomes rapidly stronger as we take more terms in the sequence:

  cond([1 x x.^2 x.^3 x.^4 x.^5 x.^6 x.^7 x.^8 x.^9 x.^10])

%%
% By contrast a quasimatrix formed of suitably normalized Legendre polynomials has
% condition number 1, since they are orthonormal:
cond(legpoly(0:10,'norm'))

%%
% A quasimatrix of Chebyshev polynomials doesn't quite achieve
% condition number 1, but it comes close:
cond(chebpoly(0:10))

%%
% Chebyshev polynomials form an excellent basis for expansions on [-1,1], a fact
% that is the starting point of Chebfun.

%% 6.5 Other norms

%%
% The definition  norm(A) = max_x norm(A*x)/norm(x)
% makes sense in other norms besides the 2-norm, and the particularly 
% important alternatives are the 1-norm and the inf-norm.  The 1-norm of
% a column quasimatrix is the "maximum column sum", i.e., the maximum of
% the 1-norms of its columns.   In the case of our quasimatrix A,
% the maximum is attained by the first column, which has norm 2:

  norm(A,1)

%%
% The inf-norm is the "maximum row sum", which for a column quasimatrix
% corresponds to the maximum of the chebfun obtained by adding the
% absolute values of the columns.  In the case of A, the sum is
% 1+|x|+...+|x|^5, which attains its maximum value 6 at x=-1 and 1:

  norm(A,inf)

%%
% The norms of row quasimatrices are analogous, with
% norm(A',inf) = norm(A,1) and norm(A',1) = norm(A,inf).
% Like Matlab itself applied to a rectangular matrix, Chebfun
% does not define cond(A,1) or cond(A,inf) if A is a quasimatrix.

%%
% The Frobenius or Hilbert-Schmidt norm is equal to the square root of the sum of the
% squares of the singular values:

  norm(A,'fro')

%% 6.6 rank, null, orth, pinv

%%
% Chebfun also contains overloads for some further
% Matlab operations related to orthogonal matrix factorizations.
% Perhaps the most useful of these is rank(A), which computes the singular
% values of A and makes a judgement as to how many of them are significantly
% different from zero.  For example, with x still defined as before, here is an
% example showing that the functions 1, sin(x)^2, and cos(x)^2 are linearly dependent:

  B = [1 sin(x).^2 cos(x).^2];
  rank(B)

%%
% Since B is rank-deficient, is has a nontrivial nullspace, and the command
% null(B) will find an orthonormal basis for it:

  null(B)

%%
% Similarly the command orth(B) finds an orthonormal basis for the range of B,
% which in this case has dimension 2:

  orth(B)

%%
% If A is an inf x n column quasimatrix, the command pinv(A) computes the pseudoinverse
% of A, an n x inf row quasimatrix such that pinv(A)*c = A\c. 

%%
% Here is a summary of the dimensions of these objects:

  subplot(1,3,1), spy(null(B)), title null(B)
  subplot(1,3,2), spy(orth(B)), title orth(B)
  subplot(1,3,3), spy(pinv(A)), title pinv(A)


%% 6.7  References
% 
% [Abramowitz & Stegun 1972]
% M. A. Abramowitz and I. A. Stegun, eds., Handbook of Mathematical Functions with
% Formulas, Graphs, and Mathematical Tables, 9th printing, Dover, 1972.
%
% [Battles 2006] Z. Battles, Numerical Linear Algebra for
% Continuous Functions, DPhil thesis, Oxford University
% Computing Laboratory, 2006.
%
% [Battles & Trefethen 2004] Z. Battles and L. N. Trefethen,
% "An extension of Matlab to continuous functions and
% operators", SIAM Journal on Scientific Computing 25 (2004),
% 1743-1770.
%
% [de Boor 1991] C. de Boor, "An alternative approach to (the teaching
% of) rank, basis, and dimension", Linear Algebra and its Applications
% 146 (1991), 221-229.
%
% [Stewart 1998] G. W. Stewart, Afternotes Goes to Graduate School:
% Lectures on Advanced Numerical Analysis, SIAM, 1998.
%
% [Trefethen 2008] L. N. Trefethen, "Householder triangularization of
% a quasimatrix", IMA Journal on Numerical Analysis 30 (2010), 887-897.
%
% [Trefethen & Bau 1997] L. N. Trefethen and D. Bau, III, Numerical Linear
% Algebra, SIAM, 1997.


##### SOURCE END #####
--></body></html>