
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Mercer's theorem and the Karhunen-Loeve expansion</title><meta name="generator" content="MATLAB 7.14"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2012-03-30"><meta name="DC.source" content="MercerKarhunenLoeve.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, tt, code { font-size:12px; }
pre { margin:0px 0px 20px; }
pre.error { color:red; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }

  </style></head><body><div class="content"><h1>Mercer's theorem and the Karhunen-Loeve expansion</h1><!--introduction--><p>Toby Driscoll, 20th December 2011</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#4">Mercer's theorem</a></li><li><a href="#10">Karhunen-Loeve expansion</a></li></ul></div><p>(Chebfun example <a href="/tmp/chebfun_tmp/chebfun/examples/stats/MercerKarhunenLoeve.m">stats/MercerKarhunenLoeve.m</a>)</p><p>Mercer's theorem is a continuous analog of the singular-value or eigenvalue decomposition of a symmetric positive definite matrix. One of its main applications is to find convenient ways to express stochastic processes, via the Karhunen-Loeve expansion [1].</p><pre class="codeinput">LW = <span class="string">'linewidth'</span>; FS = <span class="string">'fontsize'</span>; MS = <span class="string">'markersize'</span>;
</pre><h2>Mercer's theorem<a name="4"></a></h2><p>Suppose K(s,t) is a symmetric (K(t,s)=K(s,t)), continuous, and nonnegative definite kernel function on [a,b]x[a,b]. Then Mercer's theorem asserts that there is an orthonormal set of eigenfunctions {psi_j(x)} and eigenvalues {lambda_j} such that</p><pre>  K(s,t) = sum(  lambda_j psi_j(s) psi_j(t),  j=1..infinity  ),</pre><p>where the values and functions satisfy the integral eigenvalue equation</p><pre>  lambda_j psi_j(s) = int(  K(s,t) psi_j(t), t=a..b  ).</pre><p>For example, suppose we have an exponentially decaying kernel:</p><pre class="codeinput">K = @(s,t) exp(-abs(s-t));
</pre><p>We can create the integral operator and find the leading terms of its Mercer decomposition numerically.</p><pre class="codeinput">F = fred( K, domain([-1,1]) );
[Psi,Lambda] = eigs(F,20,<span class="string">'lm'</span>);
plot(Psi(:,[1 2 5 10]),LW,2), title(<span class="string">'Four Mercer eigenfunctions'</span>)
</pre><img vspace="5" hspace="5" src="MercerKarhunenLoeve_01.png" alt=""> <p>The eigenfunctions returned by eigs are orthonormal.</p><pre class="codeinput">Psi(:,1:6)'*Psi(:,1:6)
</pre><pre class="codeoutput">ans =
  Columns 1 through 3
   1.000000000000000   0.000000000000002   0.000000026818899
   0.000000000000002   1.000000000000000   0.000000000000001
   0.000000026818899   0.000000000000001   1.000000000000000
  -0.000000000000000   0.000000032098278  -0.000000000000003
   0.000000094733929   0.000000000000001   0.000000011521606
   0.000000000000001   0.000000083028756  -0.000000000000001
  Columns 4 through 6
  -0.000000000000000   0.000000094733929   0.000000000000001
   0.000000032098278   0.000000000000001   0.000000083028756
  -0.000000000000003   0.000000011521606  -0.000000000000001
   1.000000000000000  -0.000000000000001   0.000000018925274
  -0.000000000000001   0.999999999999999   0.000000000000002
   0.000000018925274   0.000000000000002   1.000000000000000
</pre><p>The truncation of the Mercer sum does lead to an underestimate of the values of the kernel K(s,t). For our example, we should get K(s,s)=1, but we get noticeably less.</p><pre class="codeinput">Psi(0,:)*Lambda*Psi(0,:)'
Psi(0.95,:)*Lambda*Psi(0.95,:)'
</pre><pre class="codeoutput">ans =
   0.979857329683123
ans =
   0.982464694653203
</pre><p>In fact, the eigenvalues decrease only like O(1/n^2), which makes the pointwise convergence in the number of terms rather slow.</p><pre class="codeinput">diff(log(diag(Lambda)))' ./ diff(log((1:20)))
</pre><pre class="codeoutput">ans =
  Columns 1 through 3
  -1.555646283238670  -2.248663437902514  -2.362606105193702
  Columns 4 through 6
  -2.344013276487799  -2.305388107579013  -2.268319088803652
  Columns 7 through 9
  -2.236247834937635  -2.208919255360176  -2.185442602570997
  Columns 10 through 12
  -2.164965338901021  -2.146795988026114  -2.130397815739643
  Columns 13 through 15
  -2.115357876991441  -2.101357150110396  -2.088146837397857
  Columns 16 through 18
  -2.075530637487911  -2.063351724075133  -2.051483201102641
  Column 19
  -2.039821069702878
</pre><h2>Karhunen-Loeve expansion<a name="10"></a></h2><p>Now suppose that X(t,omega) is a stochastic process for t in some interval [a,b] and omega in some probability space. The process is often characterized by its mean, mu(t), and its covariance, K(s,t), the expected value of (X(s)-mu(s))*(X(t)-mu(t)). Using Mercer's theorem on K, we can express the process by the K-L expansion</p><pre class="language-matlab">X(t,omega) = mu(t) + sum( sqrt(lambda_j) psi_j(t) Z_j(omega), j=1..inf )
</pre><p>where lambda_j and psi_j are Mercer eigenmodes for K, and the Z_j are uncorrelated and of unit variance.</p><p>K-L is a generalization of the singular value decomposition of a matrix, which can be written as a sum of outer products of vectors. The covariance K plays the role of the Gram matrix inner products (in probability) of "columns" of the process for different values of s and t. A number of SVD results have K-L analogs, most notably that the best approximation of the process results from truncating the expansion, if the eigenvalues are arranged in nonincreasing order.</p><p>Because the Z_j in the expansion are uncorrelated, the variance of X is just the sum of the eigenvalues. This is the trace of K, which is the integral of K(s,s); in this case, the result is 2. But we can also calculuate the variance in a truncation of the expansion by summing only some of the eigenvalues. For example, suppose the process X has the exponential covariance in K above. The eigenvalues show that 95% of the variance in the process is captured by the first 10 K-L modes:</p><pre class="codeinput">lambda = diag(Lambda);
sum(lambda(1:10)) / 2
</pre><pre class="codeoutput">ans =
   0.957887547062563
</pre><p>We can find realizations of X by selecting the random parameters Z_j in the expansion.</p><pre class="codeinput">Z = randn(10,400);
X = Psi(:,1:10)*(sqrt(Lambda(1:10,1:10))*Z);
plot(X(:,1:40))
mu = sum(X,2)/400;
hold <span class="string">on</span>, plot(mu,<span class="string">'k'</span>,LW,3)
title(<span class="string">'Random realizations, and the mean'</span>)
</pre><img vspace="5" hspace="5" src="MercerKarhunenLoeve_02.png" alt=""> <p>We should get roughly the original covariance function back.</p><pre class="codeinput">[S,T] = meshgrid(-1:.05:1);
C = cov( X(-1:.05:1,:)' );  <span class="comment">% covariance at discrete locations</span>
clf, mesh(S,T,C)
hold <span class="string">on</span>, plot3(S,T,K(S,T),<span class="string">'k.'</span>,MS,10)
</pre><img vspace="5" hspace="5" src="MercerKarhunenLoeve_03.png" alt=""> <p>If we shorten the correlation length of the process relative to the domain (i.e., more randomness), the amount of variance captured by the first 10 modes will decrease.</p><pre class="codeinput">K = @(s,t) exp(-4*abs(s-t));       <span class="comment">% decrease correlation faster, then...</span>
F = fred( K, domain([-1,1]) );
lambda = eigs(F,24,<span class="string">'lm'</span>);
sum(lambda(1:10)) / 2          <span class="comment">% ... a smaller fraction is captured</span>
</pre><pre class="codeoutput">ans =
   0.835854015697241
</pre><p>References:</p><p>[1] D. Xu, Numerical Methods for Stochastic Computations, Princeton University Press, 2010.</p><p class="footer"><br>
      Published with MATLAB&reg; 7.14<br></p></div><!--
##### SOURCE BEGIN #####
%% Mercer's theorem and the Karhunen-Loeve expansion
% Toby Driscoll, 20th December 2011

%%
% (Chebfun example <a href="/tmp/chebfun_tmp/chebfun/examples/stats/MercerKarhunenLoeve.m">stats/MercerKarhunenLoeve.m</a>)

%%
% Mercer's theorem is a continuous analog of the singular-value or
% eigenvalue decomposition of a symmetric positive definite matrix. One of
% its main applications is to find convenient ways to express stochastic
% processes, via the Karhunen-Loeve expansion [1]. 
% 

%%
%  
LW = 'linewidth'; FS = 'fontsize'; MS = 'markersize';

%% Mercer's theorem
% Suppose K(s,t) is a symmetric (K(t,s)=K(s,t)), continuous, and
% nonnegative definite kernel function on [a,b]x[a,b]. Then Mercer's
% theorem asserts that there is an orthonormal set of eigenfunctions
% {psi_j(x)} and eigenvalues {lambda_j} such that
%
%    K(s,t) = sum(  lambda_j psi_j(s) psi_j(t),  j=1..infinity  ),
%
% where the values and functions satisfy the integral eigenvalue equation
%
%    lambda_j psi_j(s) = int(  K(s,t) psi_j(t), t=a..b  ).
%

%%
% For example, suppose we have an exponentially decaying kernel:
K = @(s,t) exp(-abs(s-t));

%% 
% We can create the integral operator and find the leading terms of its
% Mercer decomposition numerically.
F = fred( K, domain([-1,1]) );
[Psi,Lambda] = eigs(F,20,'lm');
plot(Psi(:,[1 2 5 10]),LW,2), title('Four Mercer eigenfunctions')

%%
% The eigenfunctions returned by eigs are orthonormal. 
Psi(:,1:6)'*Psi(:,1:6)

%% 
% The truncation of the Mercer sum does lead to an underestimate of the
% values of the kernel K(s,t). For our example, we should get K(s,s)=1, but
% we get noticeably less.
Psi(0,:)*Lambda*Psi(0,:)'
Psi(0.95,:)*Lambda*Psi(0.95,:)'

%%
% In fact, the eigenvalues decrease only like O(1/n^2), which makes the
% pointwise convergence in the number of terms rather slow.
diff(log(diag(Lambda)))' ./ diff(log((1:20)))


%% Karhunen-Loeve expansion
% Now suppose that X(t,omega) is a stochastic process for t in some
% interval [a,b] and omega in some probability space. The process is often
% characterized by its mean, mu(t), and its covariance, K(s,t), the
% expected value of (X(s)-mu(s))*(X(t)-mu(t)). Using Mercer's theorem on K,
% we can express the process by the K-L expansion
%
%   X(t,omega) = mu(t) + sum( sqrt(lambda_j) psi_j(t) Z_j(omega), j=1..inf )
%
% where lambda_j and psi_j are Mercer eigenmodes for K, and the Z_j are
% uncorrelated and of unit variance. 

%%
% K-L is a generalization of the singular value decomposition of a matrix,
% which can be written as a sum of outer products of vectors. The
% covariance K plays the role of the Gram matrix inner products (in
% probability) of "columns" of the process for different values of s and t.
% A number of SVD results have K-L analogs, most notably that the best
% approximation of the process results from truncating the expansion, if
% the eigenvalues are arranged in nonincreasing order.

%%
% Because the Z_j in the expansion are uncorrelated, the variance of X is
% just the sum of the eigenvalues. This is the trace of K, which is the
% integral of K(s,s); in this case, the result is 2. But we can also
% calculuate the variance in a truncation of the expansion by summing only
% some of the eigenvalues. For example, suppose the process X has the
% exponential covariance in K above. The eigenvalues show that 95% of the
% variance in the process is captured by the first 10 K-L modes:
lambda = diag(Lambda); 
sum(lambda(1:10)) / 2

%%
% We can find realizations of X by selecting the random parameters Z_j in
% the expansion.
Z = randn(10,400);
X = Psi(:,1:10)*(sqrt(Lambda(1:10,1:10))*Z); 
plot(X(:,1:40))
mu = sum(X,2)/400;  
hold on, plot(mu,'k',LW,3)
title('Random realizations, and the mean')

%%
% We should get roughly the original covariance function back.
[S,T] = meshgrid(-1:.05:1);
C = cov( X(-1:.05:1,:)' );  % covariance at discrete locations
clf, mesh(S,T,C)
hold on, plot3(S,T,K(S,T),'k.',MS,10)

%% 
% If we shorten the correlation length of the process relative to the
% domain (i.e., more randomness), the amount of variance captured by the
% first 10 modes will decrease.
K = @(s,t) exp(-4*abs(s-t));       % decrease correlation faster, then...
F = fred( K, domain([-1,1]) );
lambda = eigs(F,24,'lm');
sum(lambda(1:10)) / 2          % ... a smaller fraction is captured

%%
% References:
%
% [1] D. Xu, Numerical Methods for Stochastic Computations, Princeton
% University Press, 2010.

##### SOURCE END #####
--></body></html>